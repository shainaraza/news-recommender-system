{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "downloading and parsing wikinews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14ttAH4iGPnp1P40bEPhWOwMx528fbUT7",
      "authorship_tag": "ABX9TyPx3lKK16bHcnC9EUpXdSIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shainaraza/wikinews-scrape/blob/master/downloading_and_parsing_wikinews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JEDpH7MyGDz",
        "colab_type": "text"
      },
      "source": [
        "*   Finding and retrieve online data\n",
        "*   Parsing XML using a SAX parser\n",
        "*  Parsing Mediawiki content using mwparserfromhell\n",
        "*  Running operations in parallel using multiprocessing and multithreading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCmp7BUBzFGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "# Parsing HTML\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# File system management\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqghVmlQzJlo",
        "colab_type": "text"
      },
      "source": [
        "# Searching for Wikipedia Dumps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FttthC1CzNBm",
        "colab_type": "code",
        "outputId": "6eda8860-4246-4309-cf59-9f7ad3c57563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#base_url = 'https://dumps.wikimedia.org/enwiki/'\n",
        "base_url = 'https://dumps.wikimedia.org/enwikinews/'\n",
        "index = requests.get(base_url).text\n",
        "soup_index = BeautifulSoup(index, 'html.parser')\n",
        "\n",
        "# Find the links that are dates of dumps\n",
        "dumps = [a['href'] for a in soup_index.find_all('a') if \n",
        "         a.has_attr('href')]\n",
        "dumps\n",
        "\n",
        "#for now i am taking april 2020, later i will take a few from all these three years for tempioral dynamics insh a ALLAH "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['../',\n",
              " '20191101/',\n",
              " '20191120/',\n",
              " '20191201/',\n",
              " '20191220/',\n",
              " '20200101/',\n",
              " '20200120/',\n",
              " '20200201/',\n",
              " '20200220/',\n",
              " '20200301/',\n",
              " '20200401/',\n",
              " 'latest/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OF7bWUrzq32",
        "colab_type": "code",
        "outputId": "f75157f6-e0fd-4a8e-be3d-e3b32696bc28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dump_url = base_url + '20200401/'\n",
        "\n",
        "# Retrieve the html\n",
        "dump_html = requests.get(dump_url).text\n",
        "dump_html[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tRdyLO64_qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to a soup\n",
        "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
        "\n",
        "# Find li elements with the class file\n",
        "soup_dump.find_all('li', {'class': 'file'}) #[:4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OycOJPrR5FV4",
        "colab_type": "code",
        "outputId": "e8362322-9400-4bcc-de00-e33932e8cd28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "files = []\n",
        "\n",
        "# Search through all files\n",
        "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
        "    text = file.text\n",
        "    # Select the relevant files\n",
        "    if 'pages-articles' in text:\n",
        "        files.append((text.split()[0], text.split()[1:]))\n",
        "        \n",
        "files[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('enwikinews-20200401-pages-articles-multistream.xml.bz2', ['45.7', 'MB']),\n",
              " ('enwikinews-20200401-pages-articles-multistream-index.txt.bz2',\n",
              "  ['1.2', 'MB']),\n",
              " ('enwikinews-20200401-pages-articles.xml.bz2', ['39.9', 'MB'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrIKmooz5got",
        "colab_type": "code",
        "outputId": "500e444c-f69b-4110-c561-30885638fecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "files_to_download = [file[0] for file in files if '.xml' in file[0]]\n",
        "files_to_download[-5:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enwikinews-20200401-pages-articles-multistream.xml.bz2',\n",
              " 'enwikinews-20200401-pages-articles.xml.bz2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CD73pDc57pG",
        "colab_type": "text"
      },
      "source": [
        "Download Wikipedia Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BQDzNQs589g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "keras_home = '/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os5gSWJHJ88a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os,sys\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "from keras.utils import get_file\n",
        "\n",
        "data_paths = []\n",
        "file_info = []\n",
        "\n",
        "# Iterate through each file\n",
        "for file in files_to_download[:5]:\n",
        "     path = keras_home + file\n",
        "   \n",
        "\n",
        "     if not os.path.exists( keras_home + file):\n",
        "        print('Downloading')\n",
        "        # If not, download the file\n",
        "        data_paths.append(get_file(keras_home +file, dump_url+file))\n",
        "       \n",
        "\n",
        "        # Find the file size in MB\n",
        "        file_size = os.stat(path).st_size / 1e6\n",
        "        file_info.append((file, file_size))\n",
        "\n",
        "     \n",
        "    # If the file is already downloaded find some information\n",
        "     else:\n",
        "        data_paths.append(path)\n",
        "        # Find the file size in MB\n",
        "        file_size = os.stat(path).st_size / 1e6\n",
        "        file_info.append((file, file_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqU0cnqd8Yii",
        "colab_type": "code",
        "outputId": "10fce807-1ee6-4f70-a672-8a6085d127eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f'There are {len(file_info)} partitions.')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2 partitions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M0o_sq78oT1",
        "colab_type": "code",
        "outputId": "16e36c2a-b934-4824-8a6c-aa69a529ebbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f\"The total size of files on disk is {file_df['size (MB)'].sum() / 1e3} GB\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total size of files on disk is 0.041848355 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU1l9Sr08z32",
        "colab_type": "code",
        "outputId": "0a264716-c20e-4f51-d8e2-665ebb676485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import bz2\n",
        "import subprocess\n",
        "\n",
        "data_path = data_paths[1]\n",
        "data_path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/enwikinews-20200401-pages-articles.xml.bz2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hkzCrv8qygQ",
        "colab_type": "code",
        "outputId": "44c6c2ed-c78c-4e0d-dbc8-f02ba56c9c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "sorted(file_info, key = lambda x: x[1], reverse = False)[:5]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('enwikinews-20200401-pages-articles.xml.bz2', 41.848355),\n",
              " ('enwikinews-20200401-pages-articles-multistream.xml.bz2', 47.892707)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8eVbqubA-VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The subprocess + bzcat approach is nearly twice as fast. Let's run this again and see what kind of data we have.\n",
        "lines = []\n",
        "\n",
        "\n",
        "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
        "                         stdin = open(data_path), \n",
        "                         stdout = subprocess.PIPE).stdout):\n",
        "    lines.append(line)\n",
        "lines\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ychx_-QJs4DS",
        "colab": {}
      },
      "source": [
        "\n",
        "lines[-160:-116]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F4fj99pBY5i",
        "colab_type": "text"
      },
      "source": [
        "**Parsing Approach\n",
        "**\n",
        "\n",
        "> In order to get useful information from this data, we have to parse it on two levels.\n",
        "1.  Extract the titles and article text from the XML\n",
        "2.   Extract relevant information from the article text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ydv8-bgCdiI",
        "colab_type": "text"
      },
      "source": [
        "Parsing XML using XML, SAX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ_G6_amBKS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xml.sax\n",
        "\n",
        "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
        "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
        "    def __init__(self):\n",
        "        xml.sax.handler.ContentHandler.__init__(self)\n",
        "        self._buffer = None\n",
        "        self._values = {}\n",
        "        self._current_tag = None\n",
        "        self._pages = []\n",
        "\n",
        "    def characters(self, content):\n",
        "        \"\"\"Characters between opening and closing tags\"\"\"\n",
        "        if self._current_tag:\n",
        "            self._buffer.append(content)\n",
        "\n",
        "    def startElement(self, name, attrs):\n",
        "        \"\"\"Opening tag of element\"\"\"\n",
        "        if name in ('title', 'text', 'timestamp'):\n",
        "            self._current_tag = name\n",
        "            self._buffer = []\n",
        "\n",
        "    def endElement(self, name):\n",
        "        \"\"\"Closing tag of element\"\"\"\n",
        "        if name == self._current_tag:\n",
        "            self._values[name] = ' '.join(self._buffer)\n",
        "        if name == 'page':\n",
        "            self._pages.append((self._values['title'], self._values['text']))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp_loIxBDt_r",
        "colab_type": "code",
        "outputId": "813395a6-0ea2-457d-8864-1006c5c31d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# Object for handling xml\n",
        "handler = WikiXmlHandler()\n",
        "\n",
        "# Parsing object\n",
        "parser = xml.sax.make_parser()\n",
        "parser.setContentHandler(handler)\n",
        "\n",
        "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
        "                         stdin = open(data_path), \n",
        "                         stdout = subprocess.PIPE).stdout):\n",
        "    parser.feed(line)\n",
        "    \n",
        " \n",
        "print([x[0] for x in handler._pages])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhTV1XpTt0w6",
        "colab_type": "code",
        "outputId": "3a06a30d-997c-489d-a60c-619c2e2b9938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "handler._pages[100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Yasser Arafat funeral representing Brazil comitive has back today',\n",
              " '#REDIRECT [[Brazilian delegation returns from Arafat funeral]] \\n \\n [[Category:Protected mainspace redirects]]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icuum0jGEasv",
        "colab_type": "code",
        "outputId": "80816663-4877-4b0f-bf24-718c2e384d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install mwparserfromhell "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mwparserfromhell in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ozrkksVh8DV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_article(title, text, timestamp):\n",
        "   wikicode = mwparserfromhell.parse(text)\n",
        "   wikilinks = [x.title.strip_code().strip() for x in wikicode.filter_wikilinks()]\n",
        "\n",
        "        # Extract external links\n",
        "   exlinks = [x.url.strip_code().strip() for x in wikicode.filter_external_links()]\n",
        "\n",
        "         # Find approximate length of article\n",
        "   text_length = len(wikicode.strip_code().strip())\n",
        "\n",
        "   return (title,  wikilinks, exlinks, timestamp, text_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QSrqNTlOAxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
        "    \"\"\"Parse through XML data using SAX\"\"\"\n",
        "    def __init__(self):\n",
        "        xml.sax.handler.ContentHandler.__init__(self)\n",
        "        self._buffer = None\n",
        "        self._values = {}\n",
        "        self._current_tag = None\n",
        "        self._news = []\n",
        "        self._article_count = 0\n",
        "        self._non_matches = []\n",
        "\n",
        "    def characters(self, content):\n",
        "        \"\"\"Characters between opening and closing tags\"\"\"\n",
        "        if self._current_tag:\n",
        "            self._buffer.append(content)\n",
        "\n",
        "    def startElement(self, name, attrs):\n",
        "        \"\"\"Opening tag of element\"\"\"\n",
        "        if name in ('title', 'text', 'timestamp'):\n",
        "            self._current_tag = name\n",
        "            self._buffer = []\n",
        "\n",
        "    def endElement(self, name):\n",
        "        \"\"\"Closing tag of element\"\"\"\n",
        "        if name == self._current_tag:\n",
        "            self._values[name] = ' '.join(self._buffer)\n",
        "\n",
        "        if name == 'page':\n",
        "            self._article_count += 1\n",
        "            # Search through the page to see if the page is a newsitem\n",
        "            news_item = process_article(**self._values )\n",
        "            # Append to the list of news\n",
        "            if news_item:\n",
        "                self._news.append(news_item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcev0lmZOPBB",
        "colab_type": "code",
        "outputId": "a6791a21-8e77-4a1c-959e-8c9c1138fe90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Object for handling xml\n",
        "handler = WikiXmlHandler()\n",
        "\n",
        "# Parsing object\n",
        "parser = xml.sax.make_parser()\n",
        "parser.setContentHandler(handler)\n",
        "\n",
        "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
        "                         stdin = open(data_path), \n",
        "                         stdout = subprocess.PIPE).stdout):\n",
        "    parser.feed(line)\n",
        " \n",
        "        \n",
        "print(f'Searched through {handler._article_count} .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searched through 99352 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f_c7E5tOUwe",
        "colab_type": "code",
        "outputId": "2e654137-e2f3-46c0-a6ad-0f567d3e089c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "handler._news[106]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Colin Powell Resigns as Secretary of State, Rice Likely Successor',\n",
              " ['Colin Powell Resigns as U.S. Secretary of State, Rice Likely Successor',\n",
              "  'Category:Protected mainspace redirects'],\n",
              " [],\n",
              " '2012-08-07T21:27:56Z',\n",
              " 122)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEGYXYYQYBme",
        "colab_type": "code",
        "outputId": "696b3c84-b073-4bf3-b33e-4162b1844e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Uncompress the file if not already uncompressed\n",
        "if not os.path.exists('/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/articles11.xml'):\n",
        "    subprocess.call([\"bzcat '/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/enwikinews-20200401-pages-articles.xml.bz2' >> '/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/articles11.xml'\"],\n",
        "                    shell = True)\n",
        "else:\n",
        "    print('Already uncompressed')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already uncompressed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrhQObpLYfqa",
        "colab_type": "code",
        "outputId": "9f64cdc0-380b-4912-9a43-22385826c44a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!wc '/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/articles11.xml'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  3704634  20903437 204994714 /content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/articles11.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXVth9JfcE2O",
        "colab_type": "code",
        "outputId": "ed0db099-9087-4401-bd82-eff13d71d191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "\n",
        "handler = WikiXmlHandler()\n",
        "\n",
        "# Parsing object\n",
        "parser = xml.sax.make_parser()\n",
        "parser.setContentHandler(handler)\n",
        "\n",
        "# Parse the entire file\n",
        "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
        "                         stdin = open(data_path), \n",
        "                         stdout = subprocess.PIPE).stdout):\n",
        "    parser.feed(line)\n",
        "news = handler._news\n",
        "\n",
        "print(f'\\nSearched through {handler._article_count} articles.')\n",
        "print(f'\\nFound {len(news)} newsbooks .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Searched through 99352 articles.\n",
            "\n",
            "Found 99352 newsbooks .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBvBPGZPcMY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import json\n",
        "\n",
        "# Save list of books\n",
        "with open('/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/p15_books.ndjson', 'wt') as fout:\n",
        "    for l in books:\n",
        "        fout.write(json.dumps(l) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrk_03chcEzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_in = []\n",
        "\n",
        "# Read in list of news\n",
        "with open('/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/p15_books.ndjson', 'rt') as fin:\n",
        "    for l in fin.readlines():\n",
        "       news_in.append(json.loads(l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gr9cTZDENPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_in\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRUhZhgOP_-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Save list of books\n",
        "with open('/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/p15_books.ndjson', 'wt') as fout:\n",
        "    for l in news:\n",
        "        fout.write(json.dumps(l) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDOYaOkFVAnX",
        "colab_type": "code",
        "outputId": "ad3ff125-717f-431a-e263-672751b9b520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "news_in[100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yasser Arafat funeral representing Brazil comitive has back today',\n",
              " ['Brazilian delegation returns from Arafat funeral',\n",
              "  'Category:Protected mainspace redirects'],\n",
              " [],\n",
              " '2012-08-07T21:16:35Z',\n",
              " 100]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I56D6o3OVm30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "import json\n",
        "\n",
        "def find_news(data_path, limit = None, save = True):\n",
        "    \"\"\"Find all the book articles from a compressed wikipedia XML dump.\n",
        "       `limit` is an optional argument to only return a set number of books.\n",
        "        If save, books are saved to partition directory based on file name\"\"\"\n",
        "\n",
        "    # Object for handling xml\n",
        "    handler = WikiXmlHandler()\n",
        "\n",
        "    # Parsing object\n",
        "    parser = xml.sax.make_parser()\n",
        "    parser.setContentHandler(handler)\n",
        "\n",
        "    # Iterate through compressed file\n",
        "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
        "                             stdin = open(data_path), \n",
        "                             stdout = subprocess.PIPE).stdout):\n",
        "        try:\n",
        "            parser.feed(line)\n",
        "        except StopIteration:\n",
        "            break\n",
        "            \n",
        "        # Optional limit\n",
        "        if limit is not None and len(handler._news) >= limit:\n",
        "            return handler._news\n",
        "    \n",
        "    if save:\n",
        "        partition_dir = '/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/partitions/'\n",
        "        # Create file name based on partition name\n",
        "        p_str = data_path.split('-')[-1].split('.')[-2]\n",
        "        out_dir = partition_dir + f'{p_str}.ndjson'\n",
        "\n",
        "        # Open the file\n",
        "        with open(out_dir, 'w') as fout:\n",
        "            # Write as json\n",
        "            for news_item in handler._news:\n",
        "                fout.write(json.dumps(news_item) + '\\n')\n",
        "        \n",
        "        print(f'{len(os.listdir(partition_dir))} files processed.', end = '\\r')\n",
        "\n",
        "    # Memory management\n",
        "    del handler\n",
        "    del parser\n",
        "    gc.collect()\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ8d1bzGWSjW",
        "colab_type": "code",
        "outputId": "a4ba13ee-0ceb-4ef2-d7b8-cc5d81766700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "partitions = [keras_home + file for file in os.listdir(keras_home) if '-pages-articles.xml' in file]\n",
        "len(partitions), partitions[-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " '/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/enwikinews-20200401-pages-articles.xml.bz2')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfcjuCs9WavT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocessing import Pool \n",
        "import tqdm \n",
        "\n",
        "# List of lists to single list\n",
        "from itertools import chain\n",
        "\n",
        "# Sending keyword arguments in map\n",
        "from functools import partial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwPQ2FivWcV1",
        "colab_type": "code",
        "outputId": "9b3097d4-4466-49f3-d183-26d87a318f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "os.cpu_count()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC7a2em1XR5C",
        "colab_type": "code",
        "outputId": "be638f01-ad65-4931-f5f3-afc196920804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create a pool of workers to execute processes\n",
        "pool = Pool(processes = 16)\n",
        "\n",
        "start = timer()\n",
        "\n",
        "# Map (service, tasks), applies function to each partition\n",
        "results = pool.map(find_news, partitions)\n",
        "\n",
        "pool.close()\n",
        "pool.join()\n",
        "\n",
        "end = timer()\n",
        "print(f'{end - start} seconds elapsed.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 files processed.\r117.88727112699962 seconds elapsed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNHJW-8rggI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(file_path):\n",
        "    \"\"\"Read in json data from `file_path`\"\"\"\n",
        "    \n",
        "    data = []\n",
        "    \n",
        "    # Open the file and load in json\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for l in fin.readlines():\n",
        "            data.append(json.loads(l))\n",
        "            \n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh_ZDtHOgn8W",
        "colab_type": "code",
        "outputId": "86e302de-07e2-4544-99a9-f615ec65e8b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from multiprocessing.dummy import Pool as Threadpool\n",
        "from itertools import chain\n",
        "\n",
        "start = timer()\n",
        "\n",
        "# List of files to read in\n",
        "saved_files = ['/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/partitions/' + x for x in os.listdir('/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/partitions/')]\n",
        "\n",
        "# Create a threadpool for reading in files\n",
        "threadpool = Threadpool(processes = 10)\n",
        "\n",
        "# Read in the files as a list of lists\n",
        "results = threadpool.map(read_data, saved_files)\n",
        "\n",
        "# Flatten the list of lists to a single list\n",
        "news_list = list(chain(*results))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print(f'Found {len(news_list)} news in {round(end - start)} seconds.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 99352 news in 2 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXj6_oD-gz1s",
        "colab_type": "code",
        "outputId": "6efcdb8c-cf99-424f-9d05-546c43662a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if not os.path.exists(os.getcwd() + '/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/found_news_filtered.ndjson'):\n",
        "    with open('/content/drive/My Drive/Shaina- DL NRS/data preprocessing/wikinews/found_news_filtered.ndjson', 'wt') as fout:\n",
        "        for news_item in news_list:\n",
        "             fout.write(json.dumps(news_item) + '\\n')\n",
        "    print('news saved.')\n",
        "else:\n",
        "    print('Files already saved.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "news saved.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}